---
- hosts: all
  tags: install-depedencies
  vars_files:
  - variables.yaml
  tasks:
  - name: Update hosts file
    blockinfile:
      path: /etc/hosts
      block: |
        {{ K8S_VIP }} {{ K8S_API }}
        {% for host in groups['all'] %}
        {{ hostvars[host].ansible_host }} {{ hostvars[host].ansible_hostname }} {{ host }}
        {% endfor %}
  - name: Install depedency
    apt:
      name: "{{ packages }}"
      update_cache: yes
      state: present
    vars:
      packages:
      - apt-transport-https
      - ca-certificates
      - curl
      - gnupg
      - lsb-release
      - nfs-common
      - glusterfs-client
  - name: Load required docker modules
    copy:
      dest: /etc/modules-load.d/docker.conf
      content: |
        overlay
        br_netfilter
    register: load_docker_modules
  - name: Load required sysctl parameters
    copy:
      dest: /etc/sysctl.d/99-kubernetes-cri.conf
      content: |
        net.bridge.bridge-nf-call-iptables  = 1
        net.bridge.bridge-nf-call-ip6tables = 1
        net.ipv4.ip_forward                 = 1
    register: load_sysctl_parameters
  - name: Apply modules and parameters
    shell: |
      modprobe overlay
      modprobe br_netfilter
      sysctl --system
    when: load_docker_modules.changed or load_sysctl_parameters.changed
  - name: Install docker
    apt:
      name: "{{ packages }}"
      state: present
      update_cache: yes
    vars:
      packages:
        - docker.io=20.10.7-0ubuntu5~20.04.2
        - containerd
  - name: Create Docker directories
    file:
      path: /etc/docker
      state: directory
  - name: Configuring Docker
    copy:
      dest: /etc/docker/daemon.json
      content: |
        {
          "exec-opts": ["native.cgroupdriver=systemd"],
          "log-driver": "json-file",
          "log-opts": {
          "max-size": "100m"
        },
          "storage-driver": "overlay2"
        }
    register: result
  - name: Enable Docker service
    systemd:
      name: docker
      enabled: yes
  - name: Reloading Docker service
    systemd:
      name: docker
      daemon_reload: yes
    when: result is changed
  - name: Restarting Docker service
    systemd:
      name: docker
      state: restarted
    when: result is changed
  - name: Remove swapfile from /etc/fstab
    mount:
      name: "{{ item }}"
      fstype: swap
      state: absent
    with_items:
      - swap
      - none
  - name: Disable swap
    command: swapoff -a
    when: ansible_swaptotal_mb > 0
  - name: Add kubernetes apt signing key
    apt_key:
      url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
      state: present
  - name: Add kubernetes apt repository
    apt_repository:
      repo: deb https://apt.kubernetes.io/ kubernetes-xenial main
      state: present
      filename: kubernetes.list
  - name: Install kubernetes packages
    apt:
      name: "{{ packages }}"
      state: present
      update_cache: yes
    vars:
      packages:
        - kubelet=1.15.9-00
        - kubeadm=1.15.9-00
        - kubectl=1.15.9-00
  - name: Disable auto update of kubernetes packages
    dpkg_selections:
      name: "{{ item }}"
      selection: hold
    loop:
      - kubelet
      - kubeadm
      - kubectl
      - containerd
      - docker.io

- hosts: masters
  tags: create-loadbalancer
  vars_files:
  - variables.yaml
  tasks:
  - name: Install loadbalancer packages
    apt:
      name: "{{ packages }}"
      update_cache: yes
      state: present
    vars:
      packages:
      - haproxy
      - keepalived
  - name: Disable auto updates for loadbalancer packages
    dpkg_selections:
      name: "{{ item }}"
      selection: hold
    loop:
      - haproxy
      - keepalived
  - name: Configuring loadbalancer
    copy:
      src: ../templates/haproxy.cfg
      dest: /etc/haproxy/haproxy.cfg
    register: configuring_loadbalancer
  - name: Apply loadbalancer configuration
    systemd:
      name: haproxy
      state: restarted
    when: configuring_loadbalancer.changed
  - name: Create group for Keepalived health check
    group:
      name: keepalived_script
      state: present
      system: true
  - name: Create Keepalived script user for executing health check
    user:
      name: keepalived_script
      group: keepalived_script
      system: true
      create_home: false
      shell: /sbin/nologin
  - name: Create Keepalived script for health check
    template:
      src: ../templates/check_apiserver.sh.j2
      dest: /etc/keepalived/check_apiserver.sh
      mode: 0755
    register: create_keepalived_script
  - name: Create Keepalived configuration for master
    template:
      src: ../templates/keepalived.conf.j2
      dest: /etc/keepalived/keepalived.conf
    register: create_keepalived_configuration
  - name: Apply keepalived configuration
    systemd:
      name: keepalived
      state: restarted
    when: create_keepalived_script.changed or create_keepalived_configuration.changed

- hosts: master001
  tags: bootstrap-cluster
  vars_files:
  - variables.yaml
  tasks:
  - name: Creating kubeadm config file
    template:
      src: ../manifests/kubeadm/kubernetes-1.15.9.yaml.j2
      dest: /root/kubeadm-config.yaml
  - name: Initialize the first control plane
    shell: kubeadm init --config /root/kubeadm-config.yaml
  - name: Configure kubelet
    shell: sed -i 's/6443/8443/g' /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf
  - name: Apply CNI calico
    tags: apply-calico
    shell: |
      mkdir -p /root/.kube
      cp -i /etc/kubernetes/admin.conf /root/.kube/config
      chown $(id -u):$(id -g) /root/.kube/config
      kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml
  - name: Configure kubernetes cluster to use haproxy frontend port
    shell: |
      kubectl -n kube-system get configmap kubeadm-config -o jsonpath='{.data.ClusterConfiguration}' > /root/kubeadm-config.yaml
      sed -i 's/6443/8443/g' /root/kubeadm-config.yaml
      kubeadm init phase upload-config kubeadm --config /root/kubeadm-config.yaml
      kubectl -n kube-system get cm kube-proxy -o yaml > /root/kube-proxy.yaml
      sed -i 's/6443/8443/g' /root/kube-proxy.yaml
      kubectl apply -f /root/kube-proxy.yaml
      kubectl -n kube-system rollout restart ds kube-proxy
      kubectl -n kube-system rollout status ds kube-proxy
      kubectl -n kube-public get cm cluster-info -o yaml > /root/cluster-info.yaml
      sed -i 's/6443/8443/g' /root/cluster-info.yaml
      kubectl apply -f /root/cluster-info.yaml
  - name: Generate join command for control-plane
    shell: echo "$(kubeadm token create --print-join-command) --control-plane --certificate-key $(kubeadm init phase upload-certs --upload-certs | grep -vw -e certificate -e Namespace)"
    register: join_command_controller
  - name: Copy join command for control-plane to local file
    local_action: copy content="{{ join_command_controller.stdout_lines[0] }}" dest="./join-command-controller"
  - name: Generate join command for workers
    command: kubeadm token create --print-join-command
    register: join_command_workers
  - name: Copy join command for workers to local file
    local_action: copy content="{{ join_command_workers.stdout_lines[0] }}" dest="./join-command-workers"

- hosts: master002
  tags: join-masters
  tasks:
  - name: Copy the join command to control plane
    copy:
      mode: 0777
      src: join-command-controller
      dest: /tmp/join-command.sh
  - name: Join the control plane node to cluster
    command: sh /tmp/join-command.sh
  - name: Copy kubeconfig
    shell: |
      mkdir -p /root/.kube
      cp -i /etc/kubernetes/admin.conf /root/.kube/config
      chown $(id -u):$(id -g) /root/.kube/config

- hosts: master003
  tags: join-masters
  tasks:
  - name: Copy the join command to control plane
    copy:
      mode: 0777
      src: join-command-controller
      dest: /tmp/join-command.sh
  - name: Join the control plane node to cluster
    command: sh /tmp/join-command.sh
  - name: Copy kubeconfig
    shell: |
      mkdir -p /root/.kube
      cp -i /etc/kubernetes/admin.conf /root/.kube/config
      chown $(id -u):$(id -g) /root/.kube/config

- hosts: workers
  tags: join-workers
  tasks:
  - name: Copy the join command to workers
    copy:
      mode: 0777
      src: join-command-workers
      dest: /tmp/join-command.sh
  - name: Join the workers node to cluster
    command: sh /tmp/join-command.sh

- hosts: master001
  vars_files:
  - variables.yaml
  tasks:
  - name: Labeling worker nodes
    tags: join-workers
    shell: for hosts in $(kubectl get nodes | grep worker | awk '{print$1}'); do kubectl label nodes $hosts type=app; kubectl label nodes $hosts node-role.kubernetes.io/worker=; done
  - name: Confirm if you want to create metallb
    tags: addons-metallb
    pause:
      prompt: "Do you want to create metallb? (yes/no)"
    register: create_metallb
  - name: Apply metallb manifest
    tags: addons-metallb
    shell: |
      kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10/manifests/namespace.yaml
      kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10/manifests/metallb.yaml
    when: create_metallb.user_input | bool
  - name: Waiting metallb controller ready
    tags: addons-metallb
    pause:
      echo: yes
      minutes: 2
    when: create_metallb.user_input | bool
  - name: Apply metallb configuration
    tags: addons-metallb
    shell: |
      wget https://raw.githubusercontent.com/metallb/metallb/v0.10/manifests/example-layer2-config.yaml -O /root/metallb-config.yaml
      sed -i 's+192.168.1.240/28+{{ METALLB_IPPOOL }}+g' /root/metallb-config.yaml
      kubectl apply -f /root/metallb-config.yaml
    when: create_metallb.user_input | bool

  - name: Cleaning manifest
    tags: cleanup
    file:
      path: "{{ item }}"
      state: absent
    with_items:
      - /root/cluster-info.yaml
      - /root/kube-proxy.yaml
      - /root/kubeadm-config.yaml
      - /root/metallb-config.yaml
      - /root/ingress-nginx-svc.yaml
      - /root/kubernetes-dashboard-svc.yaml
      - /root/dashboard-account.yaml
      - /root/kubeconfig-dashboard.sh

- hosts: localhost
  connection: local
  tags: cleanup
  tasks:
  - name: Removing join command
    file:
      path: "{{ item }}"
      state: absent
    with_items:
      - join-command-controller
      - join-command-workers